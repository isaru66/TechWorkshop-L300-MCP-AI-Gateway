---
title: '5. Track AI Consumption with Model Monitoring'
layout: default
nav_order: 6
parent: 'Exercise 04: Configure AI Gateway using Azure API Management'
---

# Exercise 05 - Track AI Consumption with Model Monitoring

## Introduction

Monitoring your AI consumption is critical for managing costs and ensuring fair resource allocation across your applications. By tracking token usage through Azure API Management policies and Application Insights, you gain visibility into how your AI services are being consumed, enabling you to make informed decisions about optimization and cost management.

## Description

In this exercise, you will configure monitoring policies in Azure API Management to track token consumption for your Azure OpenAI endpoints. You will enable Application Insights integration, configure token tracking dimensions, and test the monitoring setup by viewing real-time metrics in the Azure portal.

## Success Criteria

- You have enabled monitoring policies on your Azure OpenAI API in API Management.
- You have configured token tracking dimensions (prompt tokens, completion tokens, total tokens).
- You have successfully tested the API and viewed token consumption metrics in Application Insights.

## Learning Resources

- [Emit token metric policy](https://learn.microsoft.com/azure/api-management/azure-openai-emit-token-metric-policy)
- [Set up Azure Monitor](https://learn.microsoft.com/azure/api-management/api-management-howto-use-azure-monitor)
- [Import Azure OpenAI API to API Management](https://learn.microsoft.com/azure/api-management/azure-openai-api-from-specification)

## Key Tasks

### 01: Enable monitoring on the Azure OpenAI API

Now that you have imported your Azure OpenAI instance into API Management (from the previous exercise), you need to enable monitoring to track token consumption.

<details markdown="block">
<summary><strong>Expand this section to view the solution</strong></summary>

1. Navigate to the [Azure portal](https://portal.azure.com/) and open your API Management instance.

1. In the left menu, under **APIs**, select **APIs**.

1. Select your Azure OpenAI API (e.g., **aoai** or the name you used during import).

1. Select the **Settings** tab.

1. Scroll down to the **Application Insights** section.

1. Check the **Enable** checkbox to enable Application Insights integration.

1. Select your Application Insights instance from the dropdown (it should have been created during the deployment).

1. Leave other settings as default.

1. Select **Save** to apply the changes.

![Enable Application Insights on the API](../../media/Solution/monitor-enable.png)

</details>

### 02: Inspect and configure the API policy

After enabling monitoring, you need to verify that the token tracking policy is properly configured and add any additional dimensions you want to track.

<details markdown="block">
<summary><strong>Expand this section to view the solution</strong></summary>

1. While still in your Azure OpenAI API in API Management, select the **Design** tab.

1. You should see the policy configuration with token tracking dimensions. The policy may already include:
   - Prompt tokens
   - Completion tokens
   - Total tokens

1. If you want to add additional dimensions for tracking, you can edit the policy by:
   - Selecting the **All operations** scope
   - Selecting **</>** (Code editor) in the Inbound processing section
   - Adding or modifying the `azure-openai-emit-token-metric` policy

1. Example policy snippet:
   ```xml
   <azure-openai-emit-token-metric namespace="aoai">
       <dimension name="API ID" value="@(context.Api.Id)" />
       <dimension name="Deployment ID" value="@(context.Request.MatchedParameters["deployment-id"])" />
       <dimension name="Prompt Tokens" />
       <dimension name="Completion Tokens" />
       <dimension name="Total Tokens" />
   </azure-openai-emit-token-metric>
   ```

1. Select **Save** after making any policy changes.

![Inspect policy configuration](../../media/Solution/monitor-inspect-policy.png)

</details>

### 03: Test the API and generate token metrics

To verify that monitoring is working correctly, you need to send test requests to your Azure OpenAI API and generate token consumption data.

<details markdown="block">
<summary><strong>Expand this section to view the solution</strong></summary>

1. In your Azure OpenAI API, select the **Test** tab.

1. Select the **Creates a completion for the chat message** operation (or similar chat completion endpoint).

1. Fill in the required parameters:

   | Setting | Value | Description |
   |---------|-------|-------------|
   | **deployment-id** | `gpt-4o` | Your Azure OpenAI deployment name (verify in Azure AI Foundry) |
   | **api-version** | `2024-02-01` | A supported API version |

1. In the **Request body** section, enter the following JSON:

   ```json
   {
     "messages": [
       {
         "role": "system",
         "content": "you are a friendly assistant"
       },
       {
         "role": "user",
         "content": "how is the weather in London?"
       }
     ]
   }
   ```

1. Select **Send** to execute the request.

1. You should receive a successful response (HTTP 200) with the AI-generated completion.

1. **Run multiple test requests** with different prompts to generate enough data for meaningful metrics. Try variations like:
   - Short prompts (e.g., "Hello")
   - Longer prompts (e.g., "Explain the concept of machine learning in detail")
   - Different conversation contexts

![Test the API](../../media/Solution/monitor-test-import.png)

{: .note }
> Make sure to run at least 5-10 test requests to generate sufficient data for viewing in Application Insights. Metrics may take a few minutes to appear in the dashboard.

</details>

### 04: View token consumption metrics in Application Insights

After generating test data, you can view the token consumption metrics in Application Insights to analyze your AI usage patterns.

<details markdown="block">
<summary><strong>Expand this section to view the solution</strong></summary>

1. In your API Management instance, navigate to **Monitoring** > **Application Insights** in the left menu.

1. Select your Application Insights instance to open it.

1. In Application Insights, select **Monitoring** > **Metrics** from the left menu.

1. In the metrics dashboard, configure the following:
   - **Scope**: Your Application Insights instance (should be pre-selected)
   - **Metric Namespace**: Select **azure api management/service** from the dropdown
   - **Metric**: Add the following metrics one by one by selecting **+ Add metric**:
     - **Prompt Tokens**
     - **Completion Tokens**
     - **Total Tokens**

1. Adjust the time range to show recent data (e.g., Last 30 minutes or Last hour).

1. You should see a graph showing your token consumption across the test requests you made:
   - **Prompt Tokens**: Number of tokens in your input messages (~23 tokens for the example above)
   - **Completion Tokens**: Number of tokens in the AI-generated responses (~77 tokens)
   - **Total Tokens**: Combined total (~100 tokens)

![Metrics dashboard showing token consumption](../../media/Solution/monitor-dashboard.png)

1. **Optional**: Create a custom dashboard by selecting **Pin to dashboard** to save these metrics for ongoing monitoring.

1. **Optional**: Set up alerts by selecting **New alert rule** to get notified when token consumption exceeds specific thresholds.

{: .note }
> The metrics shown are aggregated values. You can change the aggregation type (Sum, Average, Max, etc.) and apply filters by dimensions like API ID or Deployment ID to drill down into specific usage patterns.

### Understanding the Metrics

- **Prompt Tokens**: Represents the "input cost" - tokens you send to the model
- **Completion Tokens**: Represents the "output cost" - tokens the model generates
- **Total Tokens**: The sum of prompt and completion tokens, which determines your total API usage cost

By monitoring these metrics, you can:
- **Identify cost drivers**: Which applications or endpoints consume the most tokens
- **Optimize prompts**: Reduce unnecessary token usage by refining input messages
- **Plan capacity**: Forecast future usage and budget accordingly
- **Ensure fairness**: Distribute token quotas fairly across teams or applications

</details>
